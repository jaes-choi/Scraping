{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: '좋아요'가 있는 뉴스 링크들의 리스트 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색 결과에서 뉴스 페이지로 연결되는 링크들의 list 만들기 - 검색 url로 get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 뉴스 검색하기\n",
    "# 필요한 패키지\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# '삼성전자' 키워드로 뉴스 검색한 결과를 requests로 html 가져 오기\n",
    "\n",
    "url = ''\n",
    "response = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html을 parsing해서 BeautifulSoup 적용하기\n",
    "\n",
    "soup = \n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색 결과에서 뉴스 페이지로 연결되는 링크들의 list 만들기 - find_all + 범위 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 뉴스 리스트에서 link 추출하기\n",
    "# 링크와 관련있는 tag <a>를 찾아 보기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범위 줄이기: http(s):// 조건 추가하기: href=re.compile(\"http\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범위 줄이기: http(s):// 조건 추가한 후 link들만 인쇄하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범위 줄이기: 'href'가 https://news.naver.com/main/read로 시작되는 anchor들 찾기\n",
    "\n",
    "# 출력된 link가 제대로 추출되었는지 브라우져에서 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색 결과에서 뉴스 페이지로 연결되는 링크들의 list 만들기 - news id 추출 및 df 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 뉴스 페이지로 연결되는 링크들의 list 만들기\n",
    "\n",
    "# 'https://news.naver.com/main/read'가 포함된 <a> 하나만 찾아 link 정보 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사 aid 정보 추출하기: 가장 마지막에 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출된 link, aid 정보들을 list로 만들기\n",
    "news_urls = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터프레임으로 만들어 pickle로 저장하기\n",
    "\n",
    "df_urls = pd.DataFrame\n",
    "df_urls.to_pickle('../working/df_urls.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색 결과의 다음 page 연결 버튼 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 다음 page에 해당되는 link들 수집\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 검색 page 연결 url 만들기\n",
    "url_main = \n",
    "for element in \n",
    "    print(url_main + element['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: link들을 읽을 수 있게 변환\n",
    "import urllib\n",
    "urllib.parse.unquote('?&where=news&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&cluster_rank=19&start=11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 참고: 현재 페이지와 다음페이지의 차이 확인하기\n",
    "page1 = urllib.parse.unquote(soup.find_all('a',onclick=re.compile('news_option_add_url'))[0]['href'])\n",
    "page2 = urllib.parse.unquote(soup.find_all('a',onclick=re.compile('news_option_add_url'))[1]['href'])\n",
    "# 'start' 항목이 10씩 증하가는 것을 확인 \n",
    "for e1, e2 in zip(page1.split('&'), page2.split('&')):\n",
    "    if e1!=e2:\n",
    "        print(e1, e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색 결과에서 뉴스 페이지로 연결되는 링크들의 list 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 참고: dataframe으로 만들기\n",
    "\n",
    "news_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url, aid in news_urls:\n",
    "    print(url.split('?')[0], end=' ')\n",
    "    print(re.split('&|=', url.split('?')[-1])[1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url, aid in news_urls:\n",
    "    print([url.split('?')[0]] + re.split('&|=', url.split('?')[-1])[1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for url, aid in news_urls:\n",
    "    urls.append([url.split('?')[0]] + re.split('&|=', url.split('?')[-1])[1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = ['url_main'] + re.split('&|=', url.split('?')[-1])[::2]\n",
    "\n",
    "pd.DataFrame(urls, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
